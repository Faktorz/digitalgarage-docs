[[install-config-downgrade]]
= Downgrading OpenShift
{product-author}
{product-version}
:icons: font
:experimental:
:toc: macro
:toc-title:
:prewrap!:
<<<<<<< HEAD
:description: Manual steps to revert {product-title} to a previous version following an upgrade.
=======
:description: Manual steps to revert OpenShift Enterprise to a previous version following an upgrade.
>>>>>>> openshift/online
:keywords: yum

toc::[]

== Overview

<<<<<<< HEAD
Following an {product-title}
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[upgrade], it may be desirable in
extreme cases to downgrade your cluster to a previous version. The following
sections outline the required steps for each system in a cluster to perform such
a downgrade for the {product-title} 3.2 to 3.1 downgrade path.

[WARNING]
====
These steps are currently only supported for
xref:../install_config/install/rpm_vs_containerized.adoc#install-config-install-rpm-vs-containerized[RPM-based
installations] of {product-title} and assumes downtime of the entire cluster.
====

[IMPORTANT]
====
For the {product-title} 3.1 to 3.0 downgrade path, see the
link:https://docs.openshift.com/enterprise/3.1/install_config/downgrade.html[OpenShift
Enterprise 3.1] documentation, which has modified steps.
=======
Following an OpenShift Enterprise
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[upgrade], it may be desirable in
extreme cases to downgrade your cluster to a previous version. The following
sections outline the required steps for each system in a cluster to perform such
a downgrade, currently supported for the OpenShift Enterprise 3.1 to 3.0
downgrade path.

[IMPORTANT]
====
This procedure is only intended for downgrades from OpenShift Enterprise 3.1 to
3.0. The procedure will be updated in a later revision for the OpenShift
Enterprise 3.2 to 3.1 downgrade path.
>>>>>>> openshift/online
====

[[downgrade-verifying-backups]]
== Verifying Backups

The Ansible playbook used during the
xref:../install_config/upgrading/index.adoc#install-config-upgrading-index[upgrade process] should have created
a backup of the *_master-config.yaml_* file and the etcd data directory. Ensure
these exist on your masters and etcd members:

====
----
<<<<<<< HEAD
/etc/origin/master/master-config.yaml.<timestamp>
/var/lib/origin/etcd-backup-<timestamp>
----
====

Also, back up the *_node-config.yaml_* file on each node (including masters,
which have the node component on them) with a timestamp:

====
----
/etc/origin/node/node-config.yaml.<timestamp>
=======
/etc/openshift/master/master-config.yaml.<timestamp>
/var/lib/openshift/etcd-backup-<timestamp>
>>>>>>> openshift/online
----
====

If you are using an external etcd cluster (versus the single embedded etcd), the
backup is likely created on all etcd members, though only one is required for
the recovery process.

The RPM downgrade process in a later step should create *_.rpmsave_* backups of
the following files, but it may be a good idea to keep a separate copy
regardless:

====
----
<<<<<<< HEAD
/etc/sysconfig/atomic-openshift-master
=======
/etc/sysconfig/openshift-master
>>>>>>> openshift/online
/etc/etcd/etcd.conf <1>
----
<1> Only required if using external etcd.
====

[[downgrade-shutting-down-the-cluster]]
== Shutting Down the Cluster

On all masters, nodes, and etcd members (if using an external etcd cluster),
<<<<<<< HEAD
ensure the relevant services are stopped.

On the master in a single master cluster:
=======
ensure the relevant services are stopped:
>>>>>>> openshift/online

====
----
# systemctl stop atomic-openshift-master
<<<<<<< HEAD
----
====

On each master in a multi-master cluster:

====
----
# systemctl stop atomic-openshift-master-api
# systemctl stop atomic-openshift-master-controllers
----
====


On all master and node hosts:

====
----
# systemctl stop atomic-openshift-node
----
====

On any external etcd hosts:

====
----
# systemctl stop etcd
----
====


=======
# systemctl stop atomic-openshift-node
# systemctl stop etcd <1>
----
<1> Only required if using external etcd.
====

>>>>>>> openshift/online
[[downgrade-removing-rpms]]
== Removing RPMs

On all masters, nodes, and etcd members (if using an external etcd cluster),
remove the following packages:

====
----
# yum remove atomic-openshift \
    atomic-openshift-clients \
    atomic-openshift-node \
    atomic-openshift-master \
    openvswitch \
    atomic-openshift-sdn-ovs \
    tuned-profiles-atomic-openshift-node
----
====

If you are using external etcd, also remove the *etcd* package:

====
----
# yum remove etcd
----
====

For embedded etcd, you can leave the *etcd* package installed, as the package is
only required so that the `etcdctl` command is available to issue operations in
later steps.

<<<<<<< HEAD
[[downgrade-docker]]
== Downgrading Docker

{product-title} 3.2 requires Docker 1.9.1 and also supports Docker 1.10.3,
however {product-title} 3.1 requires Docker 1.8.2.

Downgrade to Docker 1.8.2 on each host using the following steps:

. Remove all local containers and images on the host. Any pods backed by a
replication controller will be recreated.
+
[WARNING]
====
The following commands are destructive and should be used with caution.
====
+
Delete all containers:
+
----
# docker rm $(docker ps -a -q)
----
+
Delete all images:
+
----
# docker rmi $(docker images -q)
----

. Use `yum swap` (instead of `yum downgrade`) to install Docker 1.8.2:
+
----
# yum swap docker-* docker-*1.8.2
# sed -i 's/--storage-opt dm.use_deferred_deletion=true//' /etc/sysconfig/docker-storage
# systemctl restart docker
----

. You should now have Docker 1.8.2 installed and running on the host. Verify with
the following:
+
----
# docker version
Client:
 Version:      1.8.2-el7
 API version:  1.20
 Package Version: docker-1.8.2-10.el7.x86_64
[...]

# systemctl status docker
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
   Active: active (running) since Mon 2016-06-27 15:44:20 EDT; 33min ago
[...]
----

[[downgrade-reinstalling-rpms]]
== Reinstalling RPMs

Disable the {product-title} 3.3 repositories, and re-enable the 3.2
=======
[[downgrade-reinstalling-rpms]]
== Reinstalling RPMs

Disable the OpenShift Enteprise 3.1 repositories, and re-enable the 3.0
>>>>>>> openshift/online
repositories:

====
----
# subscription-manager repos \
<<<<<<< HEAD
    --disable=rhel-7-server-ose-3.3-rpms \
    --enable=rhel-7-server-ose-3.2-rpms
=======
    --disable=rhel-7-server-ose-3.1-rpms \
    --enable=rhel-7-server-ose-3.0-rpms
>>>>>>> openshift/online
----
====

On each master, install the following packages:

====
----
<<<<<<< HEAD
# yum install atomic-openshift \
    atomic-openshift-clients \
    atomic-openshift-node \
    atomic-openshift-master \
    openvswitch \
    atomic-openshift-sdn-ovs \
    tuned-profiles-atomic-openshift-node
=======
# yum install openshift \
    openshift-master \
    openshift-node \
    openshift-sdn-ovs
>>>>>>> openshift/online
----
====

On each node, install the following packages:

====
----
<<<<<<< HEAD
# yum install atomic-openshift \
    atomic-openshift-node \
    openvswitch \
    atomic-openshift-sdn-ovs \
    tuned-profiles-atomic-openshift-node
=======
# yum install openshift \
    openshift-node \
    openshift-sdn-ovs
>>>>>>> openshift/online
----
====

If using an external etcd cluster, install the following package on each etcd
member:

====
----
# yum install etcd
----
====

[[downgrading-restoring-etcd]]
== Restoring etcd

Whether using embedded or external etcd, you must first restore the etcd backup
by creating a new, single node etcd cluster. If using external etcd with
multiple members, you must then also add any additional etcd members to the
cluster one by one.

However, the details of the restoration process differ between
xref:downgrading-restoring-embedded-etcd[embedded] and
xref:downgrading-restoring-external-etcd[external] etcd. See the following
section that matches your etcd configuration and follow the relevant steps
before continuing to
xref:downgrade-bringing-openshift-services-back-online[Bringing OpenShift
Services Back Online].

<<<<<<< HEAD
Follow the xref:../admin_guide/backup_restore.adoc#cluster-restore[Cluster
Restore] procedure to restore single-member etcd clusters.

=======
>>>>>>> openshift/online
[[downgrading-restoring-embedded-etcd]]
=== Embedded etcd

Restore your etcd backup and configuration:

. Run the following on the master with the embedded etcd:
+
====
----
<<<<<<< HEAD
# ETCD_DIR=/var/lib/origin/openshift.local.etcd
# mv $ETCD_DIR /var/lib/etcd.orig
# cp -Rp /var/lib/origin/etcd-backup-<timestamp>/ $ETCD_DIR
=======
# ETCD_DIR=/var/lib/openshift/openshift.local.etcd
# mv $ETCD_DIR /var/lib/etcd.orig
# cp -Rp /var/lib/openshift/etcd-backup-<timestamp>/ $ETCD_DIR
>>>>>>> openshift/online
# chcon -R --reference /var/lib/etcd.orig/ $ETCD_DIR
# chown -R etcd:etcd $ETCD_DIR
----
====
+
[WARNING]
====
The `$ETCD_DIR` location differs between external and embedded etcd.
====

. Create the new, single node etcd cluster:
+
====
----
<<<<<<< HEAD
# etcd -data-dir=/var/lib/origin/openshift.local.etcd \
=======
# etcd -data-dir=/var/lib/openshift/openshift.local.etcd \
>>>>>>> openshift/online
    -force-new-cluster
----
====
+
Verify etcd has started successfully by checking the output from the above
<<<<<<< HEAD
command, which should look similar to the following near the end:
=======
command, which should look similar to the following at the end:
>>>>>>> openshift/online
+
====
----
[...]
<<<<<<< HEAD
2016-06-24 12:14:45.644073 I | etcdserver: starting server... [version: 2.2.5, cluster version: 2.2]
[...]
2016-06-24 12:14:46.834394 I | etcdserver: published {Name:default ClientURLs:[http://localhost:2379 http://localhost:4001]} to cluster 5580663a6e0002
=======
2016/01/8 13:24:21 etcdserver: starting server... [version: 2.1.1, cluster version: 2.1.0]
2016/01/8 13:24:22 raft: 5168c093630001 is starting a new election at term 13
2016/01/8 13:24:22 raft: 5168c093630001 became candidate at term 14
2016/01/8 13:24:22 raft: 5168c093630001 received vote from 5168c093630001 at term 14
2016/01/8 13:24:22 raft: 5168c093630001 became leader at term 14
2016/01/8 13:24:22 raft: raft.node: 5168c093630001 elected leader 5168c093630001 at term 14
2016/01/8 13:24:22 etcdserver: published {Name:default ClientURLs:[http://localhost:2379 http://localhost:4001]} to cluster 5168c093630002
>>>>>>> openshift/online
----
====

. Shut down the process by running the following from a separate terminal:
+
====
----
# pkill etcd
----
====

. Continue to xref:downgrade-bringing-openshift-services-back-online[Bringing
<<<<<<< HEAD
{product-title} Services Back Online].
=======
OpenShift Services Back Online].
>>>>>>> openshift/online

[[downgrading-restoring-external-etcd]]
=== External etcd

Choose a system to be the initial etcd member, and restore its etcd backup and
configuration:

. Run the following on the etcd host:
+
====
----
# ETCD_DIR=/var/lib/etcd/
# mv $ETCD_DIR /var/lib/etcd.orig
<<<<<<< HEAD
# cp -Rp /var/lib/origin/etcd-backup-<timestamp>/ $ETCD_DIR
=======
# cp -Rp /var/lib/openshift/etcd-backup-<timestamp>/ $ETCD_DIR
>>>>>>> openshift/online
# chcon -R --reference /var/lib/etcd.orig/ $ETCD_DIR
# chown -R etcd:etcd $ETCD_DIR
----
====
+
[WARNING]
====
The `$ETCD_DIR` location differs between external and embedded etcd.
====

. Restore your *_/etc/etcd/etcd.conf_* file from backup or *_.rpmsave_*.

. Create the new single node cluster using etcd's `--force-new-cluster`
<<<<<<< HEAD
option. You can do this with a long complex command using the values from
*_/etc/etcd/etcd.conf_*, or you can temporarily modify the *systemd* unit file
and start the service normally.
=======
option. You can do this with a long complex command using the values from the
*_/etc/etcd/etcd.conf_*, or you can temporarily modify the *systemd* file and
start the service normally.
>>>>>>> openshift/online
+
To do so, edit the *_/usr/lib/systemd/system/etcd.service_* and add
`--force-new-cluster`:
+
====
----
# sed -i '/ExecStart/s/"$/  --force-new-cluster"/' /usr/lib/systemd/system/etcd.service
# cat /usr/lib/systemd/system/etcd.service  | grep ExecStart

ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /usr/bin/etcd --force-new-cluster"
----
====
+
Then restart the *etcd* service:
+
====
----
# systemctl daemon-reload
# systemctl start etcd
----
====

. Verify the *etcd* service started correctly, then re-edit the
*_/usr/lib/systemd/system/etcd.service_* file and remove the
`--force-new-cluster` option:
+
====
----
# sed -i '/ExecStart/s/ --force-new-cluster//' /usr/lib/systemd/system/etcd.service
# cat /usr/lib/systemd/system/etcd.service  | grep ExecStart

ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /usr/bin/etcd"
----
====

. Restart the *etcd* service, then verify the etcd cluster is running correctly
<<<<<<< HEAD
and displays {product-title}'s configuration:
=======
and displays OpenShift's configuration:
>>>>>>> openshift/online
+
====
----
# systemctl daemon-reload
# systemctl restart etcd
# etcdctl --cert-file=/etc/etcd/peer.crt \
    --key-file=/etc/etcd/peer.key \
    --ca-file=/etc/etcd/ca.crt \
    --peers="https://172.16.4.18:2379,https://172.16.4.27:2379" \
    ls /
----
====

. If you have additional etcd members to add to your cluster, continue to
xref:downgrade-adding-addtl-etcd-members[Adding Additional etcd Members].
Otherwise, if you only want a single node external etcd, continue to
<<<<<<< HEAD
xref:downgrade-bringing-openshift-services-back-online[Bringing {product-title}
=======
xref:downgrade-bringing-openshift-services-back-online[Bringing OpenShift
>>>>>>> openshift/online
Services Back Online].

[[downgrade-adding-addtl-etcd-members]]
==== Adding Additional etcd Members

To add additional etcd members to the cluster, you must first adjust the default
<<<<<<< HEAD
*localhost* peer in the `*peerURLs*` value for the first member:
=======
*localhost* `*peerURLs*` for the first member:
>>>>>>> openshift/online

. Get the member ID for the first member using the `member list` command:
+
====
----
# etcdctl --cert-file=/etc/etcd/peer.crt \
    --key-file=/etc/etcd/peer.key \
    --ca-file=/etc/etcd/ca.crt \
    --peers="https://172.18.1.18:2379,https://172.18.9.202:2379,https://172.18.0.75:2379" \
    member list
----
====

<<<<<<< HEAD
. Update the value of `*peerURLs*` using the `etcdctl member update` command by
passing the member ID obtained from the previous step:
+
====
----
# etcdctl --cert-file=/etc/etcd/peer.crt \
    --key-file=/etc/etcd/peer.key \
    --ca-file=/etc/etcd/ca.crt \
    --peers="https://172.18.1.18:2379,https://172.18.9.202:2379,https://172.18.0.75:2379" \
    member update 511b7fb6cc0001 https://172.18.1.18:2380
----
====
+
Alternatively, you can use `curl`:
=======
. Update the `*peerURLs*`. In etcd 2.2 and beyond, this can be done with the
`etcdctl member update` command. However, OpenShift Enterprise 3.1 uses etcd
2.1, so you must use `curl`:
>>>>>>> openshift/online
+
====
----
# curl --cacert /etc/etcd/ca.crt \
    --cert /etc/etcd/peer.crt \
    --key /etc/etcd/peer.key \
    https://172.18.1.18:2379/v2/members/511b7fb6cc0001 \
    -XPUT -H "Content-Type: application/json" \
    -d '{"peerURLs":["https://172.18.1.18:2380"]}'
----
====

<<<<<<< HEAD
. Re-run the `member list` command and ensure the peer URLs no longer include
*localhost*.
=======
. Re-run the `member list` command and ensure the `*peerURLs*` no longer points
to *localhost*.
>>>>>>> openshift/online

. Now add each additional member to the cluster, one at a time.
+
[WARNING]
====
Each member must be fully added and brought online one at a time. When adding
each additional member to the cluster, the `*peerURLs*` list must be correct for
that point in time, so it will grow by one for each member added. The `etcdctl
member add` command will output the values that need to be set in the
*_etcd.conf_* file as you add each member, as described in the following
instructions.
====

.. For each member, add it to the cluster using the values that can be found in
that system's *_etcd.conf_* file:
+
====
----
# etcdctl --cert-file=/etc/etcd/peer.crt \
    --key-file=/etc/etcd/peer.key \
    --ca-file=/etc/etcd/ca.crt \
    --peers="https://172.16.4.18:2379,https://172.16.4.27:2379" \
    member add 10.3.9.222 https://172.16.4.27:2380

Added member named 10.3.9.222 with ID 4e1db163a21d7651 to cluster

ETCD_NAME="10.3.9.222"
ETCD_INITIAL_CLUSTER="10.3.9.221=https://172.16.4.18:2380,10.3.9.222=https://172.16.4.27:2380"
ETCD_INITIAL_CLUSTER_STATE="existing"
----
====

.. Using the environment variables provided in the output of the above `etcdctl
member add` command, edit the *_/etc/etcd/etcd.conf_* file on the member system
itself and ensure these settings match.

.. Now start etcd on the new member:
+
====
----
# rm -rf /var/lib/etcd/member
# systemctl enable etcd
# systemctl start etcd
----
====

.. Ensure the service starts correctly and the etcd cluster is now healthy:
+
====
----
# etcdctl --cert-file=/etc/etcd/peer.crt \
    --key-file=/etc/etcd/peer.key \
    --ca-file=/etc/etcd/ca.crt \
    --peers="https://172.16.4.18:2379,https://172.16.4.27:2379" \
    member list

51251b34b80001: name=10.3.9.221 peerURLs=https://172.16.4.18:2380 clientURLs=https://172.16.4.18:2379
d266df286a41a8a4: name=10.3.9.222 peerURLs=https://172.16.4.27:2380 clientURLs=https://172.16.4.27:2379

# etcdctl --cert-file=/etc/etcd/peer.crt \
    --key-file=/etc/etcd/peer.key \
    --ca-file=/etc/etcd/ca.crt \
    --peers="https://172.16.4.18:2379,https://172.16.4.27:2379" \
    cluster-health

cluster is healthy
member 51251b34b80001 is healthy
member d266df286a41a8a4 is healthy
----
====

.. Now repeat this process for the next member to add to the cluster.

. After all additional etcd members have been added, continue to
<<<<<<< HEAD
xref:downgrade-bringing-openshift-services-back-online[Bringing {product-title}
Services Back Online].

[[downgrade-bringing-openshift-services-back-online]]
== Bringing {product-title} Services Back Online

On each {product-title} master, restore your master and node configuration from
backup and enable and restart all relevant services.

On the master in a single master cluster:

====
----
# cp /etc/sysconfig/atomic-openshift-master.rpmsave /etc/sysconfig/atomic-openshift-master
# cp /etc/origin/master/master-config.yaml.<timestamp> /etc/origin/master/master-config.yaml
# cp /etc/origin/node/node-config.yaml.<timestamp> /etc/origin/node/node-config.yaml
# systemctl enable atomic-openshift-master
# systemctl enable atomic-openshift-node
# systemctl start atomic-openshift-master
# systemctl start atomic-openshift-node
----
====

On each master in a multi-master cluster:

====
----
# cp /etc/sysconfig/atomic-openshift-master-api.rpmsave /etc/sysconfig/atomic-openshift-master-api
# cp /etc/sysconfig/atomic-openshift-master-controllers.rpmsave /etc/sysconfig/atomic-openshift-master-controllers
# cp /etc/origin/master/master-config.yaml.<timestamp> /etc/origin/master/master-config.yaml
# cp /etc/origin/node/node-config.yaml.<timestamp> /etc/origin/node/node-config.yaml
# systemctl enable atomic-openshift-master-api
# systemctl enable atomic-openshift-master-controllers
# systemctl enable atomic-openshift-node
# systemctl start atomic-openshift-master-api
# systemctl start atomic-openshift-master-controllers
# systemctl start atomic-openshift-node
----
====

On each {product-title} node, restore your *_node-config.yaml_* file from backup
and enable and restart the *atomic-openshift-node* service:

====
----
# cp /etc/origin/node/node-config.yaml.<timestamp> /etc/origin/node/node-config.yaml
# systemctl enable atomic-openshift-node
# systemctl start atomic-openshift-node
----
====

Your {product-title} cluster should now be back online.

[[verifying-the-downgrade]]
== Verifying the Downgrade

To verify the downgrade, first check that all nodes are marked as *Ready*:

====
----
# oc get nodes
NAME                        STATUS                     AGE
master.example.com          Ready,SchedulingDisabled   165d
node1.example.com           Ready                      165d
node2.example.com           Ready                      165d
----
====

Then, verify that you are running the expected versions of the *docker-registry*
and *router* images, if deployed:

====
----
ifdef::openshift-enterprise[]
# oc get -n default dc/docker-registry -o json | grep \"image\"
    "image": "openshift3/ose-docker-registry:v3.1.1.6",
# oc get -n default dc/router -o json | grep \"image\"
    "image": "openshift3/ose-haproxy-router:v3.1.1.6",
----
====

You can use the diagnostics tool on the master to look for common issues and
provide suggestions. In {product-title} 3.1, the `oc adm diagnostics` tool is
available as `openshift ex diagnostics`:

====
----
# openshift ex diagnostics
...
[Note] Summary of diagnostics execution:
[Note] Completed with no errors or warnings seen.
----
====
=======
xref:downgrade-bringing-openshift-services-back-online[Bringing OpenShift
Services Back Online].

[[downgrade-bringing-openshift-services-back-online]]
== Bringing OpenShift Services Back Online

On each OpenShift master, restore your *openshift-master* configuration from
backup and restart relevant services:

====
----
# cp /etc/sysconfig/openshift-master.rpmsave /etc/sysconfig/openshift-master
# cp /etc/openshift/master/master-config.yaml.2015-11-20\@08\:36\:51~ /etc/openshift/master/master-config.yaml
# systemctl enable openshift-master
# systemctl enable openshift-node
# systemctl start openshift-master
# systemctl start openshift-node
----
====

On each OpenShift node, enable and restart the *openshift-node* service:

====
----
# systemctl enable openshift-node
# systemctl start openshift-node
----
====

Your OpenShift cluster should now be back online.
>>>>>>> openshift/online
