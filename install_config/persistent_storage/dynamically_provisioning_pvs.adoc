[[install-config-persistent-storage-dynamically-provisioning-pvs]]
= Dynamic Provisioning and Creating Storage Classes
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
You can provision your OpenShift cluster with storage dynamically when running
in a cloud environment. The Kubernetes
xref:../../architecture/additional_concepts/storage.adoc#architecture-additional-concepts-storage[persistent volume]
framework allows administrators to provision a cluster with persistent storage
and gives users a way to request those resources without having any knowledge of
the underlying infrastructure.

Many storage types are available for use as persistent volumes in OpenShift.
While all of them can be statically provisioned by an administrator, some types
of storage can be created dynamically using an API. These types of storage can
be provisioned in an OpenShift cluster using the new and experimental dynamic
storage feature.

[IMPORTANT]
====
ifdef::openshift-enterprise[]
Dynamic provisioning of persistent volumes is currently a Technology Preview
feature, introduced in OpenShift Enterprise 3.1.1.
endif::[]
This feature is experimental and expected to change in the future as it matures
and feedback is received from users. New ways to provision the cluster are
planned and the means by which one accesses this feature is going to change.
Backwards compatibility is not guaranteed.
====

[[enabling-provisioner-plugins]]
== Enabling Provisioner Plug-ins

OpenShift provides the following _provisioner plug-ins_, which have generic
implementations for dynamic provisioning that use the cluster's configured cloud
provider's API to create new storage resources:

[options="header"]
|===

|Storage Type |Provisioner Plug-in Name |Required Cloud Configuration

|OpenStack Cinder
|`kubernetes.io/cinder`
|xref:../../install_config/configuring_openstack.adoc#install-config-configuring-openstack[Configuring for OpenStack]

|AWS Elastic Block Store (EBS)
|`kubernetes.io/aws-ebs`
|xref:../../install_config/configuring_aws.adoc#install-config-configuring-aws[Configuring for AWS]

|GCE Persistent Disk (gcePD)
|`kubernetes.io/gce-pd`
|xref:../../install_config/configuring_gce.adoc#install-config-configuring-gce[Configuring for GCE]
|In multi-zone configurations, PVs must be created in the same region/zone as
the master node. Do this by setting the
`*failure-domain.beta.kubernetes.io/region*` and
`*failure-domain.beta.kubernetes.io/zone*` PV labels to match the master node.

|Trident from NetApp
|`netapp.io/trident`
|link:https://github.com/NetApp/trident[Configuring for Trident]
|Storage orchestrator for NetApp ONTAP, SolidFire, and E-Series storage.


|link:https://www.vmware.com/support/vsphere.html[VMWare vSphere]
|`kubernetes.io/vsphere-volume`
|link:http://kubernetes.io/docs/getting-started-guides/vsphere/[Getting Started with vSphere and Kubernetes]
|

|Azure Disk
|`kubernetes.io/azure-disk`
|xref:../../install_config/configuring_azure.adoc#install-config-configuring-azure[Configuring for Azure]
|

|===

[IMPORTANT]
====
Any chosen provisioner plug-in also requires configuration for the relevant
cloud, host, or third-party provider as per the relevant documentation.
====

[[defining-storage-classes]]
== Defining a StorageClass

_StorageClass_ objects are currently a globally scoped object and need to be
created by `cluster-admin` or `storage-admin` users. There are currently five
plug-ins that are supported. The following sections describe the basic object
definition for a _StorageClass_ and specific examples for each of the supported
plug-in types.

[[basic-spec-defintion]]
=== Basic StorageClass Object Definition

.StorageClass Basic Object Definition
[source,yaml]
----
kind: StorageClass <1>
apiVersion: storage.k8s.io/v1 <2>
metadata:
  name: foo <3>
  annotations: <4>
     ...
provisioner: kubernetes.io/plug-in-type <5>
parameters: <6>
  param1: value
  ...
  paramN: value

----
<1> (required) The API object type.
<2> (required) The current apiVersion.
<3> (required) The name of the StorageClass.
<4> (optional) Annotations for the StorageClass
<5> (required) The type of provisioner associated with this storage class.
<6> (optional) The parameters required for the specific provisioner, this will change
from plug-in to plug-in.

When your OpenShift cluster is configured for EBS, GCE, or Cinder, the
associated provisioner plug-in is implied and automatically enabled. No
additional OpenShift configuration by the cluster administration is required for
dynamic provisioning.

To set a _StorageClass_ as the cluster-wide default:
----
   storageclass.kubernetes.io/is-default-class: "true"
----
This enables any Persistent Volume Claim (PVC) that does not specify a specific
volume to automatically be provisioned through the _default_ StorageClass

[NOTE]
====
Beta annotation `storageclass.beta.kubernetes.io/is-default-class` is still
working. However it will be removed in a future release.
====

To set a _StorageClass_ description:
----
   kubernetes.io/description: My StorgeClass Description
----

Future provisioner plug-ins will include the many types of storage a single
provider offers. AWS, for example, has several types of EBS volumes to offer,
each with its own performance characteristics; there is also an NFS-like storage
option. More provisioner plug-ins will be implemented for the supported storage
types available in OpenShift.

[[openstack-cinder-spec]]
=== OpenStack Cinder Object Defintion

.cinder-storageclass.yaml
[source,yaml]
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: gold
provisioner: kubernetes.io/cinder
parameters:
  type: fast  <1>
  availability: nova <2>

----
<1> VolumeType created in Cinder. Default is empty.
<2> Availability Zone. Default is empty.

Users can request dynamically provisioned storage by including a storage class
annotation in their xref:../../dev_guide/persistent_volumes.adoc#dev-guide-persistent-volumes[persistent
volume claim]:

.aws-ebs-storageclass.yaml
[source,yaml]
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: slow
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1 <1>
  zone: us-east-1d <2>
  iopsPerGB: "10" <3>
  encrypted: true <4>
  kmsKeyId: keyvalue <5>

----

<1> Select from `io1`, `gp2`, `sc1`, `st1`. The default is `gp2`. link:http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html[See AWS docs for valid ARN value].
<2> AWS zone. If not specified, the zone is randomly selected from zones where {product-title} cluster has a node.
<3> Only for io1 volumes. I/O operations per second per GiB. The AWS volume plug-in multiplies this with the size of the requested volume to compute IOPS of the volume. The value cap is 20,000 IOPS, which is the maximum supported by AWS. See AWS documentation for further details.
<4> Denotes whether to encrypt the EBS volume. Valid values are `true` or `false`.
<5> (optional) The full Amazon Resource Name (ARN) of the key to use when encrypting the volume. If none is supplied but encrypted is true, AWS generates a key. link:http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html[See AWS docs for valid ARN value].

[[gce-persistentdisk-gcePd]]
=== GCE PersistentDisk (gcePD) Object Defintion

.gce-pd-storageclass.yaml
[source,yaml]
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard  <1>
  zone: us-central1-a  <2>
----
<1> Select either `pd-standard` or `pd-ssd`. The default is `pd-ssd`.
<2> GCE zone. If not specified, the zone is randomly chosen from zones in the same region as `controller-manager`.


[[glusterfs]]
=== GlusterFS Object Defintion

.glusterfs-storageclass.yaml
[source,yaml]
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: slow
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://127.0.0.1:8081" <1>
  restuser: "admin" <2>
  secretName: "heketi-secret" <3>
  secretNamespace: "default" <4>
  gidMin: "40000" <5>
  gidMax: "50000" <6>
----
<1> Gluster REST service/Heketi service URL that provisions Gluster
volumes on demand. The general format should be
`{http/https}://{IPaddress}:{Port}`. This is a mandatory parameter for the
GlusterFS dynamic provisioner. If the Heketi service is exposed as a routable
service in the {product-title}, it will have a resolvable fully qualified domain
name and Heketi service URL. For additional information and configuration, See
link:https://access.redhat.com/documentation/en/red-hat-gluster-storage/3.1/single/container-native-storage-for-openshift-container-platform/[Container-Native
Storage for OpenShift Container Platform].
<2> Gluster REST service/Heketi user who has access to create
volumes in the Gluster Trusted Pool.
<3> Identification of a Secret instance that contains a user password to use when
talking to the Gluster REST service. Optional; an empty password will be used
when both `secretNamespace` and `secretName` are omitted. The provided secret
must be of type `"kubernetes.io/glusterfs"`.
<4> The namespace of mentioned `secretName`. Optional; an empty password will be used
when both `secretNamespace` and `secretName` are omitted. The provided secret
must be of type `"kubernetes.io/glusterfs"`.
<5> Optional. The minimum value of GID range for the storage class.
<6> Optional. The maximum value of GID range for the storage class.

|JSON-in-description
|GCE PD
|====

[[ceph-persistentdisk-cephRBD]]
=== Ceph RBD Object Definition

.ceph-storageclass.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 10.16.153.105:6789  <1>
  adminId: kube  <2>
  adminSecretName: ceph-secret  <3>
  adminSecretNamespace: kube-system  <4>
  pool: kube  <5>
  userId: kube  <6>
  userSecretName: ceph-secret-user  <7>

----
<1> Ceph monitors, comma delimited. It is required.
<2> Ceph client ID that is capable of creating images in the pool. Default is "admin".
<3> Secret Name for `adminId`. It is required. The provided secret must have type "kubernetes.io/rbd".
<4> The namespace for `adminSecret`. Default is "default".
<5> Ceph RBD pool. Default is "rbd".
<6> Ceph client ID that is used to map the Ceph RBD image. Default is the same as `adminId`.
<7> The name of Ceph Secret for `userId` to map Ceph RBD image. It must exist in the same namespace as PVCs. It is required.

[[trident]]
=== Trident Object Definition

.trident.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gold
provisioner: netapp.io/trident <1>
parameters: <2>
  media: "ssd"
  provisioningType: "thin"
  snapshots: "true"

----
Trident uses the parameters as selection criteria for the different pools of
storage that are registered with it. Trident itself is configured separately.

<1> For more information about installing Trident with {product-title}, see the link:https://github.com/NetApp/trident[Trident documentation].
<2> For more information about supported parameters, see the link:https://github.com/NetApp/trident#storage-attributes[storage attributes] section of the Trident documentation.

[[vsphere]]
=== VMWare vSphere Object Definition

.vsphere-storageclass.yaml
[source,yaml]
----
kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
  name: slow
provisioner: kubernetes.io/vsphere-volume <1>
parameters:
  diskformat: thin <2>

----
<1> For more information about using VMWare vSphere with {product-title}, see the link:https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/index.html[VMWare vSphere documentation].
<2>  `diskformat`: `thin`, `zeroedthick` and `eagerzeroedthick`. See vSphere docs for details. Default: `thin`

[[azure-unmanaged-disk]]
=== Azure Unmanaged Disk Object Definition

.azure-unmanaged-disk-storageclass.yaml
[source,yaml]
----
  kind: StorageClass
  apiVersion: storage.k8s.io/v1
  metadata:
    name: slow
  provisioner: kubernetes.io/azure-disk
  parameters:
    skuName: Standard_LRS  <1>
    location: eastus  <2>
    storageAccount: azure_storage_account_name  <3>
----
<1> Azure storage account SKU tier. Default is empty.
<2> Azure storage account location. Default is empty.
<3> Azure storage account name. This must reside in the same resource group as the cluster. If a storage account is specified, the `location` is ignored. If a storage account is not specified, a new storage account gets created in the same resource group as the cluster.

[[azure-advanced-disk]]
=== Advanced Azure Disk Object Definition

.azure-advanced-disk-storageclass.yaml
[source,yaml]
----
  kind: StorageClass
  apiVersion: storage.k8s.io/v1
  metadata:
    name: slow
  provisioner: kubernetes.io/azure-disk
  parameters:
    storageaccounttype: Standard_LRS  <1>
    kind: Shared  <2>
----
<1> Azure storage account SKU tier. Default is empty. *Note:* Premium VM can attach both _Standard_LRS_ and _Premium_LRS_ disks, Standard VM can only attach _Standard_LRS_ disks, Managed VM can only attach managed disks, and unmanaged VM can only attach unmanaged disks.
<2> possible values are `shared` (default), `dedicated`, and `managed`. When `kind` is `shared`, all unmanaged disks are created in a few shared storage accounts in the same resource group as the cluster. When `kind` is `dedicated`, a new dedicated storage account gets created for the new unmanaged disk in the same resource group as the cluster. When `kind` is `managed`, a new managed disk gets created.


[[change-default-storage-class]]
== Changing the Default StorageClass
If you are using GCE and AWS, use the following process to change the default StorageClass:

. List the StorageClass:
+

====
----
$ oc get storageclass

NAME                 TYPE
gp2 (default)        kubernetes.io/aws-ebs <1>
standard             kubernetes.io/gce-pd
----
<1> `(default)` denotes the default StorageClass.
====

. Change the value of the annotation `storageclass.kubernetes.io/is-default-class` to `false` for the default StorageClass:
+
[cols="1,1"]
|====
|Project Name |PVC Name

|app-server
|a-pv-01

|
|a-pv-02

|notifications
|n-pv-01
|====
+
The storage administrators search for the orphaned volumes,
matching project names and PVC names to the
`kubernetes.io/created-for/pvc/namespace` and
`kubernetes.io/created-for/pvc/name` tags, respectively.
They find them and arrange to make them available again for data-recovery efforts.

- The users do not explicitly delete the dynamically provisioned storage
volumes when they are finished with a project.
The storage administrators find the defunct volumes and delete them.
Unlike the preceding scenario, they need match only the project names
to `kubernetes.io/created-for/pvc/namespace`.


[[dynamic-pvs-volume-recycling]]
== Volume Recycling

Volumes created dynamically by a provisioner have their
`*persistentVolumeReclaimPolicy*` set to *Delete*. When a persistent volume
claim is deleted, its backing persistent volume is considered released of its
claim, and that resource can be reclaimed by the cluster. Dynamic provisioning
utilizes the provider's API to delete the volume from the provider and then
removes the persistent volume from the cluster.
